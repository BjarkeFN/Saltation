{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/storage/bjarke/GenBank/\"\n",
    "fastafile = datadir + \"aligned.fasta\"\n",
    "tsvfile = datadir + \"metadata.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tsv metadata file into a pandas DataFrame:\n",
    "df = pd.read_csv(tsvfile, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-thailand",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df_USA = df.loc[df['country'] == \"USA\"]\n",
    "df_USA = df.loc[df['country'] == \"United Kingdom\"]\n",
    "#df_USA = df.loc[df['region'] == \"Europe\"]\n",
    "#df_USA = df.loc[df['region'] == \"Asia\"]\n",
    "\n",
    "# Filter to get only human sequences ...\n",
    "\n",
    "df_USA = df_USA.loc[df_USA['host'] == \"Homo sapiens\"]\n",
    "\n",
    "# Only accept those that have overall quality \"good\"\n",
    "\n",
    "df_USA = df_USA.loc[df_USA['QC_overall_status'] == \"good\"]\n",
    "\n",
    "print(f\"Matched {len(df_USA)} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple way to measure Hamming distance only from the tsv data:\n",
    "\n",
    "def dist_from_subs(subs0, subs1):\n",
    "    subs0 = str(subs0).split(\",\")\n",
    "    subs1 = str(subs1).split(\",\")\n",
    "\n",
    "    dist = 0\n",
    "\n",
    "    for sub in subs0:\n",
    "        if sub in subs1:\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "\n",
    "    for sub in subs1:\n",
    "        if sub in subs0:\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "\n",
    "    return dist\n",
    "\n",
    "def dist_from_subs_and_dels(subs0, subs1, dels0, dels1):\n",
    "    subs0 = str(subs0).split(\",\")\n",
    "    subs1 = str(subs1).split(\",\")\n",
    "    dels0_prelim = str(dels0).split(\",\")\n",
    "    dels1_prelim = str(dels1).split(\",\")\n",
    "    \n",
    "    dels0_true = []\n",
    "    dels1_true = []\n",
    "    \n",
    "    for dp in dels0_prelim:\n",
    "        if \"-\" in dp: # It's a range\n",
    "            dpsplit = dp.split(\"-\")\n",
    "            dp_beg_end = (int(dpsplit[0]), int(dpsplit[1]))\n",
    "            dpdiff = dp_beg_end[1]-dp_beg_end[0]\n",
    "            for i in range(0,dpdiff+1):\n",
    "                dels0_true.append(deepcopy(dp_beg_end[0]) + i)\n",
    "        else: # It's a single deletion (or none)\n",
    "            if dp=='nan':\n",
    "                pass\n",
    "            else:\n",
    "                dels0_true.append(int(deepcopy(dp)))\n",
    "    \n",
    "    for dp in dels1_prelim:\n",
    "        if \"-\" in dp: # It's a range\n",
    "            dpsplit = dp.split(\"-\")\n",
    "            dp_beg_end = (int(dpsplit[0]), int(dpsplit[1]))\n",
    "            dpdiff = dp_beg_end[1]-dp_beg_end[0]\n",
    "            for i in range(0,dpdiff+1):\n",
    "                dels1_true.append(deepcopy(dp_beg_end[0]) + i)\n",
    "        else: # It's a single deletion (or none)\n",
    "            if dp=='nan':\n",
    "                pass\n",
    "            else:\n",
    "                dels1_true.append(deepcopy(int(dp)))\n",
    "\n",
    "    d0 = dels0_true\n",
    "    d1 = dels1_true\n",
    "            \n",
    "    dist = 0\n",
    "\n",
    "    for sub in subs0:\n",
    "        if sub in subs1 or sub=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "\n",
    "    for sub in subs1:\n",
    "        if sub in subs0 or sub=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "\n",
    "    for d in d0:\n",
    "        if d in d1:\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "\n",
    "    for d in d1:\n",
    "        if d in d0:\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "            \n",
    "    return dist\n",
    "\n",
    "def dist_from_nuc(seq1, seq2):\n",
    "    # Compute also the Hamming distance between the two:\n",
    "    bases = [\"G\", \"C\", \"T\", \"A\"]\n",
    "    ndiff = 0\n",
    "    print(\"Length before padding removal:\", len(seq1))\n",
    "    # Remove padding (at both ends!)\n",
    "    seq1_new = str(seq1)\n",
    "    seq2_new = str(seq2)\n",
    "    # First, remove left\n",
    "    for i in range(len(seq1_new)):\n",
    "        if seq1_new[0]==\"-\" and seq2_new[0]==\"-\":\n",
    "            seq1_new = seq1_new[1:]\n",
    "            seq2_new = seq1_new[1:]\n",
    "        else:\n",
    "            break\n",
    "    # Then right:\n",
    "    L_tmp = len(seq1_new)\n",
    "    while True:\n",
    "        if seq1_new[-1]==\"-\" and seq2_new[-1]==\"-\":\n",
    "            seq1_new = seq1_new[:-1]\n",
    "            seq2_new = seq1_new[:-1]\n",
    "        else:\n",
    "            break\n",
    "    seq1 = seq1_new\n",
    "    seq2 = seq2_new\n",
    "    \n",
    "    for i in range(len(seq1)):\n",
    "        if (seq1[i] in bases) and (seq2[i] in bases):\n",
    "            print(seq1[i], \"vs\", seq2[i])\n",
    "            if seq1[i] != seq2[i]:\n",
    "                print(\"Difference at\", i)\n",
    "                ndiff += 1\n",
    "    return ndiff\n",
    "\n",
    "def dist_from_nuc_simple(seq1, seq2, verbose=False):\n",
    "    # Compute also the Hamming distance between the two:\n",
    "    bases = [\"G\", \"C\", \"T\", \"A\"]\n",
    "    ndiff = 0\n",
    "    seq1 = str(seq1)\n",
    "    seq2 = str(seq2)\n",
    "    \n",
    "    for i in range(len(seq1)):\n",
    "        if seq1[i] != seq2[i]:\n",
    "            if (seq1[i] in bases) and (seq2[i] in bases):\n",
    "                if verbose:\n",
    "                    print(\"Difference (sub) at \", i+1, seq1[i], \"vs\", seq2[i])\n",
    "                ndiff += 1\n",
    "            elif (seq1[i] in bases) or (seq2[i] in bases):\n",
    "                usable = True\n",
    "                # Check that not all previous entries are \"-\" in either string:\n",
    "                if seq1[i]==\"N\" or seq2[i]==\"N\":\n",
    "                    usable = False\n",
    "                if len(seq1[0:i].replace(\"-\",\"\"))==0 or len(seq2[0:i].replace(\"-\",\"\"))==0:\n",
    "                    usable = False\n",
    "                if len(seq1[i:].replace(\"-\",\"\"))==0 or len(seq2[i:].replace(\"-\",\"\"))==0:\n",
    "                    usable = False\n",
    "                if usable:\n",
    "                    if verbose:\n",
    "                        print(\"Difference (del) at\", i+1, seq1[i], \"vs\", seq2[i])\n",
    "                    ndiff += 1\n",
    "    return ndiff\n",
    "\n",
    "# A rudimentary way to measure Hamming distance only from the tsv data:\n",
    "def diffs_site(subs0, subs1, dels0, dels1):\n",
    "    subs0 = str(subs0).split(\",\")\n",
    "    subs1 = str(subs1).split(\",\")\n",
    "    dels0_prelim = str(dels0).split(\",\")\n",
    "    dels1_prelim = str(dels1).split(\",\")\n",
    "    \n",
    "    dels0_true = []\n",
    "    dels1_true = []\n",
    "    \n",
    "    for dp in dels0_prelim:\n",
    "        if \"-\" in dp: # It's a range\n",
    "            dpsplit = dp.split(\"-\")\n",
    "            dp_beg_end = (int(dpsplit[0]), int(dpsplit[1]))\n",
    "            dpdiff = dp_beg_end[1]-dp_beg_end[0]\n",
    "            for i in range(0,dpdiff+1):\n",
    "                dels0_true.append(deepcopy(dp_beg_end[0]) + i)\n",
    "        else: # It's a single deletion (or none)\n",
    "            if dp=='nan':\n",
    "                pass\n",
    "            else:\n",
    "                dels0_true.append(int(deepcopy(dp)))\n",
    "    \n",
    "    for dp in dels1_prelim:\n",
    "        if \"-\" in dp: # It's a range\n",
    "            dpsplit = dp.split(\"-\")\n",
    "            dp_beg_end = (int(dpsplit[0]), int(dpsplit[1]))\n",
    "            dpdiff = dp_beg_end[1]-dp_beg_end[0]\n",
    "            for i in range(0,dpdiff+1):\n",
    "                dels1_true.append(deepcopy(dp_beg_end[0]) + i)\n",
    "        else: # It's a single deletion (or none)\n",
    "            if dp=='nan':\n",
    "                pass\n",
    "            else:\n",
    "                dels1_true.append(deepcopy(int(dp)))\n",
    "\n",
    "    d0 = dels0_true\n",
    "    d1 = dels1_true\n",
    "            \n",
    "    dist = []\n",
    "\n",
    "    for sub in subs0:\n",
    "        if sub in subs1 or sub=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            # Strip bases and convert to integer\n",
    "            sub_s = re.sub(\"[^0-9]\", \"\", sub)\n",
    "            try:\n",
    "                sub_s = int(sub_s)\n",
    "            except ValueError:\n",
    "                print(\"Error, sub was:\", sub)\n",
    "            dist.append(sub_s)\n",
    "\n",
    "    for sub in subs1:\n",
    "        if sub in subs0 or sub=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            # Strip bases and convert to integer\n",
    "            sub_s = re.sub(\"[^0-9]\", \"\", sub)\n",
    "            try:\n",
    "                sub_s = int(sub_s)\n",
    "            except ValueError:\n",
    "                print(\"Error, sub was:\", sub)\n",
    "            dist.append(sub_s)\n",
    "\n",
    "    for d in d0:\n",
    "        if d in d1:\n",
    "            pass\n",
    "        else:\n",
    "            dist.append(d)\n",
    "\n",
    "    for d in d1:\n",
    "        if d in d0:\n",
    "            pass\n",
    "        else:\n",
    "            dist.append(d)\n",
    "            \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_dists(df_match, downsample=False):\n",
    "    if downsample:\n",
    "        comparisons = 50\n",
    "    else:\n",
    "        comparisons = 5000\n",
    "    comps_done = 0\n",
    "    dists = []\n",
    "    while comps_done < comparisons:\n",
    "        i = random.randint(0,len(df_match)-1)\n",
    "        j = random.randint(0,len(df_match)-1)\n",
    "        while i==j:\n",
    "            j = random.randint(0,len(df_match)-1)\n",
    "        #dist_loc = dist_from_subs(df_match.iloc[i].substitutions,df_match.iloc[j].substitutions)\n",
    "        dist_loc = dist_from_subs_and_dels(df_match.iloc[i].substitutions,df_match.iloc[j].substitutions,df_match.iloc[i].deletions,df_match.iloc[j].deletions)\n",
    "        dists.append(dist_loc)\n",
    "        comps_done += 1\n",
    "    return dists\n",
    "\n",
    "def get_avg_dists_two_sets(df1, df2, downsample=False):\n",
    "    if downsample:\n",
    "        comparisons = 50\n",
    "    else:\n",
    "        comparisons = 5000\n",
    "    comps_done = 0\n",
    "    dists = []\n",
    "    while comps_done < comparisons:\n",
    "        i = random.randint(0,len(df1)-1)\n",
    "        j = random.randint(0,len(df2)-1)\n",
    "        dist_loc = dist_from_subs_and_dels(df1.iloc[i].substitutions,df2.iloc[j].substitutions,df1.iloc[i].deletions,df2.iloc[j].deletions)\n",
    "        dists.append(dist_loc)\n",
    "        comps_done += 1\n",
    "    return dists\n",
    "\n",
    "def get_avg_dists_site(df_match, downsample=False):\n",
    "    if downsample:\n",
    "        comparisons = 50\n",
    "    else:\n",
    "        comparisons = 1000\n",
    "    comps_done = 0\n",
    "    dists = []\n",
    "    while comps_done < comparisons:\n",
    "        i = random.randint(0,len(df_match)-1)\n",
    "        j = random.randint(0,len(df_match)-1)\n",
    "        while i==j:\n",
    "            j = random.randint(0,len(df_match)-1)\n",
    "        #dist_loc = dist_from_subs(df_match.iloc[i].substitutions,df_match.iloc[j].substitutions)\n",
    "        diffs_loc = diffs_site(df_match.iloc[i].substitutions,df_match.iloc[j].substitutions,df_match.iloc[i].deletions,df_match.iloc[j].deletions)\n",
    "        dists = dists + diffs_loc\n",
    "        comps_done += 1\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/insert/storage/directory/here/\"\n",
    "datadir = datadir + \"UK/\"\n",
    "timespan = 875 # days\n",
    "\n",
    "distributions = []\n",
    "dateranges = []\n",
    "\n",
    "savefigs = True\n",
    "savedata = True\n",
    "downsample = False\n",
    "\n",
    "# Filter by date\n",
    "date_beg = datetime.datetime(2020, 3, 1)\n",
    "#date_beg = datetime.datetime(2022, 1, 1)\n",
    "\n",
    "# Set reference sequences:\n",
    "df_ref = df_USA.loc[df_USA['date'] >= \"2021-03-15\"]\n",
    "df_ref = df_ref.loc[df_ref['date'] < \"2021-03-22\"]\n",
    "\n",
    "for i in range(timespan):\n",
    "    date_beg_str = date_beg.strftime('%Y-%m-%d')\n",
    "    date_end = date_beg\n",
    "    date_end += datetime.timedelta(days=7)\n",
    "    date_end_str = date_end.strftime('%Y-%m-%d')\n",
    "    df_match = df_USA.loc[df_USA['date'] >= date_beg_str]\n",
    "    df_match = df_match.loc[df_match['date'] < date_end_str]\n",
    "    print(f\"{date_beg_str}. Rows in filtered dataframe:\", round(len(df_match)/1e3,2), \"x 10^3\" )\n",
    "    if len(df_match) < 10:\n",
    "        print(\"Skipping due to missing data! Fiducially adding [0] distribution.\")\n",
    "        distributions.append([0,0,0,0,0,0,0,0,0])\n",
    "        dateranges.append((date_beg_str,date_end_str))\n",
    "    else:\n",
    "        #dists = get_avg_dists(df_match, downsample=downsample)\n",
    "        dists = get_avg_dists_two_sets(df_match, df_ref, downsample=downsample)\n",
    "        if savefigs:\n",
    "            __, __, __ = plt.hist(dists, range=[0, 150], bins=151, density=True)\n",
    "            plt.xlim([0,150])\n",
    "            plt.ylim([0,0.1])\n",
    "            plt.xlabel(\"Hamming distance\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(date_beg_str)\n",
    "            plt.savefig(datadir + str(i) + \".png\", dpi=175, facecolor='white', transparent=False, bbox_inches='tight', pad_inches=0)\n",
    "            plt.clf()\n",
    "        distributions.append(dists)\n",
    "        dateranges.append((date_beg_str,date_end_str))\n",
    "    date_beg += datetime.timedelta(days=1)\n",
    "if savedata:\n",
    "    datadict = dict()\n",
    "    datadict[\"hammingdistributions\"] = distributions\n",
    "    datadict[\"dateranges\"] = dateranges\n",
    "    rand_ID = str(random.randint(100000000,999999999))\n",
    "    filename = \"data_\" + rand_ID\n",
    "    pklname = filename + \".pkl\"\n",
    "    f = open(datadir + pklname, \"wb\")\n",
    "    pickle.dump(datadict,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelized version! \n",
    "from multiprocessing import Pool\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def generate_heatmaps_parready(paramdict):\n",
    "    date_beg_str = paramdict[\"date_beg_str\"]\n",
    "    date_end_str = paramdict[\"date_end_str\"]\n",
    "    df_ref = paramdict[\"df_ref\"]\n",
    "    i = paramdict[\"i\"]\n",
    "    df_USA = paramdict[\"df\"]\n",
    "    print(\"i:\", i)\n",
    "    \n",
    "    savefigs = False\n",
    "    downsample = False\n",
    "\n",
    "    df_match = df_USA.loc[df_USA['date'] >= date_beg_str]\n",
    "    df_match = df_match.loc[df_match['date'] < date_end_str]\n",
    "    if len(df_match) < 10:\n",
    "        dists = [0,0,0,0,0,0,0,0,0]\n",
    "    else:\n",
    "        #dists = get_avg_dists(df_match, downsample=downsample)\n",
    "        dists = get_avg_dists_two_sets(df_match, df_ref, downsample=downsample)\n",
    "        if savefigs:\n",
    "            plt.ioff()\n",
    "            fontsize=10\n",
    "            font = {'family' : 'sans',\n",
    "                    'weight' : 'normal',\n",
    "                    'size'   : fontsize}\n",
    "            mystyle = 'seaborn'\n",
    "            plt.style.use(mystyle)\n",
    "            plt.rc('font', **font)\n",
    "\n",
    "            __, __, __ = plt.hist(dists, range=[0, 150], bins=151, density=True)\n",
    "            plt.xlim([0,150])\n",
    "            plt.ylim([0,0.1])\n",
    "            plt.xlabel(\"Hamming distance\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(date_beg_str)\n",
    "            plt.savefig(datadir + str(i) + \".png\", dpi=175, facecolor='white', transparent=False, bbox_inches='tight', pad_inches=0)\n",
    "            plt.clf()\n",
    "    return dists\n",
    "\n",
    "datadir = \"/insert/storage/directory/here/\"\n",
    "datadir = datadir + \"UK_vsBA2/\"\n",
    "\n",
    "#datadir = datadir + \"UK_test/\"\n",
    "#timespan = 2*365 # days\n",
    "#timespan = 10 # days\n",
    "timespan = 875 # days\n",
    "\n",
    "# Set reference sequences:\n",
    "# Alpha:\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2021-03-15\"]\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2021-03-22\"]\n",
    "# Delta:\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2021-09-20\"]\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2021-09-27\"]\n",
    "# Omicron (BA1):\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2022-01-01\"]\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2022-01-08\"]\n",
    "# Omicron BA2:\n",
    "df_ref = df_USA.loc[df_USA['date'] >= \"2022-04-09\"]\n",
    "df_ref = df_ref.loc[df_ref['date'] < \"2022-04-16\"]\n",
    "\n",
    "# Filter by date\n",
    "date_beg = datetime.datetime(2020, 3, 1)\n",
    "#date_beg = datetime.datetime(2022, 1, 1)\n",
    "\n",
    "\n",
    "distributions = []\n",
    "dateranges = []\n",
    "\n",
    "\n",
    "# First create list of parameter dictionaries:\n",
    "# Generate all the starting dates:\n",
    "date_beg_strs = []\n",
    "date_end_strs = []\n",
    "for i in range(timespan):\n",
    "    date_beg_str = date_beg.strftime('%Y-%m-%d')\n",
    "    date_end = date_beg\n",
    "    date_end += datetime.timedelta(days=7)\n",
    "    date_end_str = date_end.strftime('%Y-%m-%d')\n",
    "    date_beg_strs.append(date_beg_str)\n",
    "    date_end_strs.append(date_end_str)\n",
    "    date_beg += datetime.timedelta(days=1)\n",
    "\n",
    "paramdicts = []\n",
    "\n",
    "for i in range(len(date_beg_strs)):\n",
    "    paramdicts.append({\"date_beg_str\":date_beg_strs[i], \"date_end_str\":date_end_strs[i], \"df_ref\": df_ref, \"i\": i, \"df\": df_USA}  )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        pool = Pool(processes=30) # How many concurrent processes? \n",
    "        outarr = pool.map(generate_heatmaps_parready, paramdicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Populate datadict on the basis of parallell runs:\n",
    "# First, generate the begdates list of 2-tuples:\n",
    "\n",
    "savedata = True\n",
    "\n",
    "dateranges = []\n",
    "distributions = []\n",
    "\n",
    "for i in range(len(date_beg_strs)):\n",
    "    dateranges.append((date_beg_strs[i],date_end_strs[i]))\n",
    "    distributions.append(outarr[i])\n",
    "    \n",
    "\n",
    "datadict = dict()\n",
    "datadict[\"hammingdistributions\"] = distributions\n",
    "datadict[\"dateranges\"] = dateranges\n",
    "\n",
    "if savedata:\n",
    "    rand_ID = str(random.randint(100000000,999999999))\n",
    "    filename = \"data_\" + rand_ID\n",
    "    pklname = filename + \".pkl\"\n",
    "    f = open(datadir + pklname, \"wb\")\n",
    "    pickle.dump(datadict,f)\n",
    "    f.close()\n",
    "    print(\"Saved to:\", datadir + pklname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamming distance at each site!\n",
    "# (parallel)\n",
    "from multiprocessing import Pool\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def generate_site_heatmaps_parready(paramdict):\n",
    "    date_beg_str = paramdict[\"date_beg_str\"]\n",
    "    date_end_str = paramdict[\"date_end_str\"]\n",
    "    df_ref = paramdict[\"df_ref\"]\n",
    "    i = paramdict[\"i\"]\n",
    "    df_USA = paramdict[\"df\"]\n",
    "    print(\"i:\", i)\n",
    "    \n",
    "    savefigs = False\n",
    "    downsample = False\n",
    "\n",
    "    df_match = df_USA.loc[df_USA['date'] >= date_beg_str]\n",
    "    df_match = df_match.loc[df_match['date'] < date_end_str]\n",
    "    if len(df_match) < 10:\n",
    "        dists = [0,0,0,0,0,0,0,0,0]\n",
    "    else:\n",
    "        dists = get_avg_dists_site(df_match, downsample=downsample)\n",
    "    return dists\n",
    "\n",
    "datadir = \"/insert/storage/directory/here/\"\n",
    "#datadir = datadir + \"UK_downsample/\"\n",
    "#datadir = datadir + \"UK_vsAlpha/\"\n",
    "#datadir = datadir + \"UK_vsDelta/\"\n",
    "#datadir = datadir + \"UK_vsBA2/\"\n",
    "datadir = datadir + \"UK_site/\"\n",
    "\n",
    "#datadir = datadir + \"UK_test/\"\n",
    "#timespan = 2*365 # days\n",
    "#timespan = 10 # days\n",
    "timespan = 875 # days\n",
    "\n",
    "# Set reference sequences:\n",
    "# Alpha:\n",
    "df_ref = df_USA.loc[df_USA['date'] >= \"2021-03-15\"]\n",
    "df_ref = df_ref.loc[df_ref['date'] < \"2021-03-22\"]\n",
    "# Delta:\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2021-09-20\"]\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2021-09-27\"]\n",
    "# Omicron (BA1):\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2022-01-01\"]\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2022-01-08\"]\n",
    "# Omicron BA2:\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2022-04-09\"]\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2022-04-16\"]\n",
    "\n",
    "# Filter by date\n",
    "date_beg = datetime.datetime(2020, 3, 1)\n",
    "#date_beg = datetime.datetime(2022, 1, 1)\n",
    "\n",
    "\n",
    "distributions = []\n",
    "dateranges = []\n",
    "\n",
    "\n",
    "# First create list of parameter dictionaries:\n",
    "# Generate all the starting dates:\n",
    "date_beg_strs = []\n",
    "date_end_strs = []\n",
    "for i in range(timespan):\n",
    "    date_beg_str = date_beg.strftime('%Y-%m-%d')\n",
    "    date_end = date_beg\n",
    "    date_end += datetime.timedelta(days=7)\n",
    "    date_end_str = date_end.strftime('%Y-%m-%d')\n",
    "    date_beg_strs.append(date_beg_str)\n",
    "    date_end_strs.append(date_end_str)\n",
    "    date_beg += datetime.timedelta(days=1)\n",
    "\n",
    "paramdicts = []\n",
    "\n",
    "for i in range(len(date_beg_strs)):\n",
    "    paramdicts.append({\"date_beg_str\":date_beg_strs[i], \"date_end_str\":date_end_strs[i], \"df_ref\": df_ref, \"i\": i, \"df\": df_USA}  )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        pool = Pool(processes=30) # How many concurrent processes? \n",
    "        outarr_site = pool.map(generate_site_heatmaps_parready, paramdicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Populate datadict on the basis of parallell runs:\n",
    "# First, generate the begdates list of 2-tuples:\n",
    "\n",
    "savedata = True\n",
    "\n",
    "dateranges = []\n",
    "distributions = []\n",
    "\n",
    "for i in range(len(date_beg_strs)):\n",
    "    dateranges.append((date_beg_strs[i],date_end_strs[i]))\n",
    "    distributions.append(outarr_site[i])\n",
    "    \n",
    "\n",
    "datadict_site = dict()\n",
    "datadict_site[\"hammingdistributions\"] = distributions\n",
    "datadict_site[\"dateranges\"] = dateranges\n",
    "\n",
    "if savedata:\n",
    "    rand_ID = str(random.randint(100000000,999999999))\n",
    "    filename = \"data_site_\" + rand_ID\n",
    "    pklname = filename + \".pkl\"\n",
    "    f = open(datadir + pklname, \"wb\")\n",
    "    pickle.dump(datadict_site,f)\n",
    "    f.close()\n",
    "    print(\"Saved to:\", datadir + pklname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Heatmap matrix as well\n",
    "x_max = 30000\n",
    "bins = 300\n",
    "ny = len(distributions) # time\n",
    "nx = bins\n",
    "plotmat_sites = np.zeros((ny,nx))\n",
    "for i in range(ny):\n",
    "    hist_n, hist_bins = np.histogram(distributions[i], density=False, range=[0, x_max], bins=bins)\n",
    "    plotmat_sites[i,:] = deepcopy(hist_n)\n",
    "    print(f\"t={i+1} out of {ny}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot site-wise map\n",
    "\n",
    "fig, ((ax)) = plt.subplots(1, 1, dpi=175, figsize=(6,8))\n",
    "\n",
    "showmat = np.log(0.05+plotmat_sites[:,:]/np.max(plotmat_sites)) \n",
    "#showmat = plotmat_sites[:,:]/np.max(plotmat_sites)\n",
    "\n",
    "cax = ax.imshow(showmat, interpolation='nearest', aspect='auto', cmap=plt.get_cmap('inferno'))\n",
    "\n",
    "cbar = fig.colorbar(cax, ticks=[np.min(showmat), np.max(showmat)])\n",
    "cbar.ax.set_yticklabels(['0', '1'])  # vertically oriented colorbar\n",
    "cbar.set_label('Frequency (logarithmic) within generation', rotation=270)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "country_name = \"UK\"\n",
    "#country_name = \"USA\"\n",
    "#country_name = \"Europe\"\n",
    "\n",
    "startdate = \"2020-03-01\"\n",
    "\n",
    "plt.xlabel(\"Site/100\")\n",
    "plt.ylabel(f\"Day (since {startdate})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data:\n",
    "load_from = \"/insert/storage/directory/here/UK_origin/data_708266984.pkl\"\n",
    "datadict = pickle.load( open(load_from, \"rb\" ) )\n",
    "\n",
    "ddh = datadict[\"hammingdistributions\"]\n",
    "ddd = datadict[\"dateranges\"]\n",
    "means = []\n",
    "variances = []\n",
    "deciles = []\n",
    "begdates = []\n",
    "\n",
    "for i in range(1,10):\n",
    "    deciles.append([])\n",
    "i_s = []\n",
    "for i in range(len(ddh)):\n",
    "    if np.mean(ddh[i])>0:\n",
    "        means.append(np.mean(ddh[i]))\n",
    "        variances.append(np.var(ddh[i]))\n",
    "        for j in range(1,10):\n",
    "            pctile = np.percentile(ddh[i], j*10)\n",
    "            deciles[j-1].append(pctile)\n",
    "        i_s.append(i)\n",
    "        begdates.append(ddd[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = [datetime.datetime.strptime(d,\"%Y-%m-%d\").date() for d in begdates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tidx = 200\n",
    "\n",
    "fig, ((ax1)) = plt.subplots(1, 1, dpi=175, figsize=(5,3.5))\n",
    "\n",
    "ax = plt.gca()\n",
    "formatter = mdates.DateFormatter(\"%Y-%m-%d\")\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "#ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=4))\n",
    "\n",
    "locator = mdates.DayLocator()\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "\n",
    "\n",
    "dec_idx=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "fontsize=10\n",
    "\n",
    "font = {'family' : 'sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : fontsize}\n",
    "\n",
    "\n",
    "#mystyle = 'seaborn-notebook'\n",
    "mystyle = 'seaborn'\n",
    "\n",
    "# bmh is quite good somehow, but maybe too heavy\n",
    "plt.style.use(mystyle)\n",
    "plt.rc('font', **font)\n",
    "#plt.style.use(\"ggplot\")\n",
    "\n",
    "#plt.figure(figsize=(5, 4), dpi=175)\n",
    "fig, ((ax1)) = plt.subplots(1, 1, dpi=275, figsize=(6,3.5))\n",
    "\n",
    "#ax = plt.gca()\n",
    "\n",
    "# If percentage y axis is desired:\n",
    "#ax1.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "\n",
    "cm = plt.cm.get_cmap('inferno')\n",
    "#cm = plt.cm.get_cmap('Set1')\n",
    "#cm = plt.cm.get_cmap('YlGnBu_r')\n",
    "\n",
    "#dcol = 0.25\n",
    "#col0 = 0.0\n",
    "\n",
    "#dcol=0.2\n",
    "#dcol = 0.25\n",
    "dcol = 0.25\n",
    "#col0=-dcol+0.05\n",
    "col0 = 0\n",
    "\n",
    "#t = np.array(range(len(I_noss_hi)))/48\n",
    "#t_coarse = np.array(range(len(I_noss_lo)))/48\n",
    "\n",
    "#plt.plot(t, I_noss_hi+E_noss_hi, color=cm(col0 + 3*dcol), alpha=0.3)\n",
    "\n",
    "# Set colours:\n",
    "# Normal order\n",
    "col_multipliers = [1, 2, 2.8]\n",
    "# Reversed order:\n",
    "col_reversed = True\n",
    "if col_reversed:\n",
    "    col_multipliers = list(reversed(col_multipliers))\n",
    "col1 = cm(col0 + col_multipliers[0]*dcol)\n",
    "col2 = cm(col0 + col_multipliers[1]*dcol)\n",
    "col3 = cm(col0 + col_multipliers[2]*dcol)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot_date(begdates, means, fmt='', label=\"Mean\", color=col1)\n",
    "plt.plot_date(begdates, deciles[dec_idx], fmt='--', label=f\"{(dec_idx+1)*10}% percentile\", color=col2)\n",
    "\n",
    "#plt.xlabel(\"Time (generations)\")\n",
    "plt.ylabel(\"Hamming distance\")\n",
    "\n",
    "ticklist = []\n",
    "for i in range(len(begdates)):\n",
    "    if begdates[i][-2:]==\"01\":\n",
    "        ticklist.append(begdates[i])\n",
    "        print(\"Adding tick\", ticklist[-1])\n",
    "plt.xticks(ticklist, rotation=60)\n",
    "\n",
    "print(\"First day:\", begdates[0])\n",
    "print(\"Last day:\", begdates[-1])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim([-1,850])\n",
    "print(begdates[850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN VS VARIANCE\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig, ((ax1)) = plt.subplots(1, 1, dpi=275, figsize=(6,3.5))\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "varmultiply = 0.08\n",
    "\n",
    "plt.plot_date(begdates, means, fmt='', label=\"Mean\", color=col1)\n",
    "plt.plot_date(begdates, varmultiply*np.array(variances), fmt='--', label=f\"Variance scaled by {varmultiply}\", color=col2)\n",
    "\n",
    "#plt.xlabel(\"Time (generations)\")\n",
    "plt.ylabel(\"Hamming distance\")\n",
    "\n",
    "#plt.ylim([0,100])\n",
    "\n",
    "ticklist = []\n",
    "for i in range(len(begdates)):\n",
    "    if begdates[i][-2:]==\"01\":\n",
    "        ticklist.append(begdates[i])\n",
    "plt.xticks(ticklist, rotation=60)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to matrix:\n",
    "\n",
    "x_max = 200\n",
    "ny = len(ddh) # time\n",
    "nx = x_max\n",
    "plotmat = np.zeros((ny,nx))\n",
    "for i in range(ny):\n",
    "    hist_n, hist_bins = np.histogram(ddh[i], density=True, range=[0, x_max], bins=x_max)\n",
    "    plotmat[i,:] = deepcopy(hist_n)\n",
    "    print(f\"t={i+1} out of {ny}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax)) = plt.subplots(1, 1, dpi=175, figsize=(6,8))\n",
    "\n",
    "showmat = np.log(0.02+plotmat[0:800,:175]) \n",
    "\n",
    "cax = ax.imshow(showmat, interpolation='nearest', aspect='auto', cmap=plt.get_cmap('inferno'))\n",
    "\n",
    "cbar = fig.colorbar(cax, ticks=[np.min(showmat), np.max(showmat)])\n",
    "cbar.ax.set_yticklabels(['0', '1'])  # vertically oriented colorbar\n",
    "cbar.set_label('Frequency (logarithmic) within generation', rotation=270)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "country_name = \"UK\"\n",
    "#country_name = \"USA\"\n",
    "#country_name = \"Europe\"\n",
    "\n",
    "startdate = \"2020-03-01\"\n",
    "\n",
    "plt.xlabel(\"Hamming distance\")\n",
    "plt.ylabel(f\"Day (since {startdate})\")\n",
    "plt.title(f\"Time-dependent Hamming heatmap ({country_name})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
