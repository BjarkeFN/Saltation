{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/storage/user/GenBank/\"\n",
    "tsvfile = datadir + \"metadata_dec01.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tsv metadata file into a pandas DataFrame\n",
    "df = pd.read_csv(tsvfile, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "df # Get a quick overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-thailand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filtering the data:\n",
    "\n",
    "#country = \"United Kingdom\"\n",
    "country = \"USA\"\n",
    "df_USA = df.loc[df['country'] == country]\n",
    "\n",
    "#country = \"World\"\n",
    "#df_USA = df\n",
    "\n",
    "#country = \"NonEurNA\" # World except Europe and North America\n",
    "#df_USA = df.loc[df['region'] != \"Europe\"] # Filter Europe out\n",
    "#df_USA = df_USA.loc[df_USA['region'] != \"North America\"] # Filter NA out\n",
    "\n",
    "#df_USA = df.loc[df['country'] == \"USA\"]\n",
    "#df_USA = df.loc[df['country'] == \"United Kingdom\"]\n",
    "#df_USA = df.loc[df['region'] == \"Europe\"]\n",
    "#df_USA = df.loc[df['region'] == \"Asia\"]\n",
    "\n",
    "# Filter to get only human sequences ...\n",
    "\n",
    "df_USA = df_USA.loc[df_USA['host'] == \"Homo sapiens\"]\n",
    "\n",
    "# Only accept those that have overall quality \"good\"\n",
    "\n",
    "df_USA = df_USA.loc[df_USA['QC_overall_status'] == \"good\"]\n",
    "\n",
    "print(f\"Matched {len(df_USA)} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_USA # We call the filtered dataframe df_USA in every case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple way to measure Hamming distance only from the tsv data:\n",
    "def dist_from_subs_dels_ins(subs0, subs1, dels0, dels1, ins0, ins1):\n",
    "    subs0 = str(subs0).split(\",\")\n",
    "    subs1 = str(subs1).split(\",\")\n",
    "    dels0_prelim = str(dels0).split(\",\")\n",
    "    dels1_prelim = str(dels1).split(\",\")\n",
    "    \n",
    "    # Structure of insertions is e.g.\n",
    "    # 2820:GCT,19230:ACGT\n",
    "    # if no insertions are present, it is nan.\n",
    "    \n",
    "    ins0 = str(ins0).split(\",\")\n",
    "    ins0_true = []\n",
    "    for ip in ins0:\n",
    "        if ip=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            ip = ip.split(\":\")\n",
    "            pos_beg = int(ip[0])\n",
    "            nucs = ip[1]\n",
    "            pos = pos_beg\n",
    "            for nuc in nucs:\n",
    "                ins0_true.append((pos,nuc))\n",
    "                pos += 1\n",
    "    i0 = ins0_true\n",
    "    \n",
    "    ins1 = str(ins1).split(\",\")\n",
    "    ins1_true = []\n",
    "    for ip in ins1:\n",
    "        if ip=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            ip = ip.split(\":\")\n",
    "            pos_beg = int(ip[0])\n",
    "            nucs = ip[1]\n",
    "            pos = pos_beg\n",
    "            for nuc in nucs:\n",
    "                ins1_true.append((pos,nuc))\n",
    "                pos += 1\n",
    "    i1 = ins1_true\n",
    "    \n",
    "    dels0_true = []\n",
    "    dels1_true = []\n",
    "    \n",
    "    for dp in dels0_prelim:\n",
    "        if \"-\" in dp: # It's a range\n",
    "            dpsplit = dp.split(\"-\")\n",
    "            dp_beg_end = (int(dpsplit[0]), int(dpsplit[1]))\n",
    "            dpdiff = dp_beg_end[1]-dp_beg_end[0]\n",
    "            for i in range(0,dpdiff+1):\n",
    "                dels0_true.append(deepcopy(dp_beg_end[0]) + i)\n",
    "        else: # It's a single deletion (or none)\n",
    "            if dp=='nan':\n",
    "                pass\n",
    "            else:\n",
    "                dels0_true.append(int(deepcopy(dp)))\n",
    "    \n",
    "    for dp in dels1_prelim:\n",
    "        if \"-\" in dp: # It's a range\n",
    "            dpsplit = dp.split(\"-\")\n",
    "            dp_beg_end = (int(dpsplit[0]), int(dpsplit[1]))\n",
    "            dpdiff = dp_beg_end[1]-dp_beg_end[0]\n",
    "            for i in range(0,dpdiff+1):\n",
    "                dels1_true.append(deepcopy(dp_beg_end[0]) + i)\n",
    "        else: # It's a single deletion (or none)\n",
    "            if dp=='nan':\n",
    "                pass\n",
    "            else:\n",
    "                dels1_true.append(deepcopy(int(dp)))\n",
    "\n",
    "    d0 = dels0_true\n",
    "    d1 = dels1_true\n",
    "            \n",
    "    dist = 0\n",
    "\n",
    "    for sub in subs0:\n",
    "        if sub in subs1 or sub=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "\n",
    "    for sub in subs1:\n",
    "        if sub in subs0 or sub=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "\n",
    "    for d in d0:\n",
    "        if d in d1:\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "\n",
    "    for d in d1:\n",
    "        if d in d0:\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "    \n",
    "    for i in i0:\n",
    "        if i in i1:\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "    \n",
    "    for i in i1:\n",
    "        if i in i0:\n",
    "            pass\n",
    "        else:\n",
    "            dist += 1\n",
    "            \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_dists(df_match, downsample=False):\n",
    "    if downsample:\n",
    "        comparisons = 50\n",
    "    else:\n",
    "        comparisons = 5000\n",
    "    comps_done = 0\n",
    "    dists = []\n",
    "    while comps_done < comparisons:\n",
    "        i = random.randint(0,len(df_match)-1)\n",
    "        j = random.randint(0,len(df_match)-1)\n",
    "        while i==j:\n",
    "            j = random.randint(0,len(df_match)-1)\n",
    "        dist_loc = dist_from_subs_dels_ins(df_match.iloc[i].substitutions,df_match.iloc[j].substitutions,df_match.iloc[i].deletions,df_match.iloc[j].deletions,df_match.iloc[i].insertions,df_match.iloc[j].insertions)\n",
    "        \n",
    "        dists.append(dist_loc)\n",
    "        comps_done += 1\n",
    "    return dists\n",
    "\n",
    "def get_avg_dists_two_sets(df1, df2, downsample=False):\n",
    "    if downsample:\n",
    "        comparisons = 50\n",
    "    else:\n",
    "        comparisons = 5000\n",
    "    comps_done = 0\n",
    "    dists = []\n",
    "    while comps_done < comparisons:\n",
    "        i = random.randint(0,len(df1)-1)\n",
    "        j = random.randint(0,len(df2)-1)\n",
    "        dist_loc = dist_from_subs_dels_ins(df1.iloc[i].substitutions,df2.iloc[j].substitutions,df1.iloc[i].deletions,df2.iloc[j].deletions,df1.iloc[i].insertions,df2.iloc[j].insertions)\n",
    "        dists.append(dist_loc)\n",
    "        comps_done += 1\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelized Hamming distribution computations:\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def generate_heatmaps_parready(paramdict):\n",
    "    date_beg_str = paramdict[\"date_beg_str\"]\n",
    "    date_end_str = paramdict[\"date_end_str\"]\n",
    "    df_ref = paramdict[\"df_ref\"]\n",
    "    i = paramdict[\"i\"]\n",
    "    df_USA = paramdict[\"df\"]\n",
    "    print(\"i:\", i)\n",
    "    \n",
    "    savefigs = False\n",
    "    downsample = False\n",
    "\n",
    "    df_match = df_USA.loc[df_USA['date'] >= date_beg_str]\n",
    "    df_match = df_match.loc[df_match['date'] < date_end_str]\n",
    "    if len(df_match) < 10:\n",
    "        dists = [0,0,0,0,0,0,0,0,0]\n",
    "    else:\n",
    "        dists = get_avg_dists(df_match, downsample=downsample)\n",
    "        #dists = get_avg_dists_two_sets(df_match, df_ref, downsample=downsample)\n",
    "        if savefigs:\n",
    "            plt.ioff()\n",
    "            fontsize=10\n",
    "            font = {'family' : 'sans',\n",
    "                    'weight' : 'normal',\n",
    "                    'size'   : fontsize}\n",
    "            mystyle = 'seaborn'\n",
    "            plt.style.use(mystyle)\n",
    "            plt.rc('font', **font)\n",
    "\n",
    "            __, __, __ = plt.hist(dists, range=[0, 150], bins=151, density=True)\n",
    "            plt.xlim([0,150])\n",
    "            plt.ylim([0,0.1])\n",
    "            plt.xlabel(\"Hamming distance\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(date_beg_str)\n",
    "            plt.savefig(datadir + str(i) + \".png\", dpi=175, facecolor='white', transparent=False, bbox_inches='tight', pad_inches=0)\n",
    "            plt.clf()\n",
    "    return dists\n",
    "\n",
    "def generate_heatmaps_parready_preread(paramdict):\n",
    "    date_beg_str = paramdict[\"date_beg_str\"]\n",
    "    date_end_str = paramdict[\"date_end_str\"]\n",
    "    df_ref = paramdict[\"df_ref\"]\n",
    "    df_match = paramdict[\"df_match\"]\n",
    "    i = paramdict[\"i\"]\n",
    "    print(\"i:\", i)\n",
    "    \n",
    "    savefigs = False\n",
    "    downsample = False\n",
    "\n",
    "    \n",
    "    if len(df_match) < 10:\n",
    "        dists = [0,0,0,0,0,0,0,0,0]\n",
    "    else:\n",
    "        dists = get_avg_dists(df_match, downsample=downsample)\n",
    "        #dists = get_avg_dists_two_sets(df_match, df_ref, downsample=downsample)\n",
    "        if savefigs:\n",
    "            plt.ioff()\n",
    "            fontsize=10\n",
    "            font = {'family' : 'sans',\n",
    "                    'weight' : 'normal',\n",
    "                    'size'   : fontsize}\n",
    "            mystyle = 'seaborn'\n",
    "            plt.style.use(mystyle)\n",
    "            plt.rc('font', **font)\n",
    "\n",
    "            __, __, __ = plt.hist(dists, range=[0, 150], bins=151, density=True)\n",
    "            plt.xlim([0,150])\n",
    "            plt.ylim([0,0.1])\n",
    "            plt.xlabel(\"Hamming distance\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(date_beg_str)\n",
    "            plt.savefig(datadir + str(i) + \".png\", dpi=175, facecolor='white', transparent=False, bbox_inches='tight', pad_inches=0)\n",
    "            plt.clf()\n",
    "    return dists\n",
    "\n",
    "datadir = \"/storage/user/GenBank/frames/\"\n",
    "datadir = datadir + country + \"_dec01/\"\n",
    "\n",
    "timespan = 1005 # days\n",
    "\n",
    "# Set a reference sequence (only used if utilizing get_avg_dists_two_sets() in generate_heatmaps_parready_preread() )\n",
    "# Alpha:\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2021-03-15\"]\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2021-03-22\"]\n",
    "# Delta:\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2021-09-20\"]\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2021-09-27\"]\n",
    "# Omicron (BA1):\n",
    "df_ref = df_USA.loc[df_USA['date'] >= \"2022-01-01\"]  # UK\n",
    "df_ref = df_ref.loc[df_ref['date'] < \"2022-01-08\"] # UK\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2022-01-17\"]  # USA\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2022-01-24\"] # USA\n",
    "# Omicron BA2:\n",
    "#df_ref = df_USA.loc[df_USA['date'] >= \"2022-04-09\"]\n",
    "#df_ref = df_ref.loc[df_ref['date'] < \"2022-04-16\"]\n",
    "\n",
    "# Starting date:\n",
    "date_beg = datetime.datetime(2020, 3, 1)\n",
    "\n",
    "distributions = []\n",
    "dateranges = []\n",
    "\n",
    "\n",
    "# First create list of parameter dictionaries:\n",
    "# Generate all the starting dates:\n",
    "date_beg_strs = []\n",
    "date_end_strs = []\n",
    "for i in range(timespan):\n",
    "    date_beg_str = date_beg.strftime('%Y-%m-%d')\n",
    "    date_end = date_beg\n",
    "    date_end += datetime.timedelta(days=7)\n",
    "    date_end_str = date_end.strftime('%Y-%m-%d')\n",
    "    date_beg_strs.append(date_beg_str)\n",
    "    date_end_strs.append(date_end_str)\n",
    "    date_beg += datetime.timedelta(days=1)\n",
    "\n",
    "\n",
    "already_prepared = False\n",
    "\n",
    "if already_prepared:\n",
    "    print(\"Data already initialized, proceeding directly to Hamming computations.\")\n",
    "    print(\"If this is not correct, abort computation immediately!\")\n",
    "    time.sleep(10)\n",
    "    print(\"Commencing!\")\n",
    "else:\n",
    "    paramdicts = []\n",
    "    for i in range(len(date_beg_strs)):\n",
    "        print(f\"Preparing data for day {i+1} out of {len(date_beg_strs)}.\")\n",
    "        paramdicts.append({\"date_beg_str\":date_beg_strs[i], \"date_end_str\":date_end_strs[i], \"df_ref\": deepcopy(df_ref), \"i\": i, \"df_match\": deepcopy(df_USA.loc[(df_USA['date'] >= date_beg_strs[i]) & (df_USA['date'] < date_end_strs[i])])}  )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        pool = Pool(processes=35) # How many concurrent processes? \n",
    "        outarr = pool.map(generate_heatmaps_parready_preread, paramdicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mkdir -p \" + '\"' + datadir + '\"')\n",
    "\n",
    "## Populate datadict on the basis of parallell runs:\n",
    "# First, generate the begdates list of 2-tuples:\n",
    "\n",
    "savedata = True\n",
    "\n",
    "dateranges = []\n",
    "distributions = []\n",
    "\n",
    "for i in range(len(date_beg_strs)):\n",
    "    dateranges.append((date_beg_strs[i],date_end_strs[i]))\n",
    "    distributions.append(outarr[i])\n",
    "    \n",
    "\n",
    "datadict = dict()\n",
    "datadict[\"hammingdistributions\"] = distributions\n",
    "datadict[\"dateranges\"] = dateranges\n",
    "\n",
    "if savedata:\n",
    "    rand_ID = str(random.randint(100000000,999999999))\n",
    "    filename = \"data_\" + rand_ID\n",
    "    pklname = filename + \".pkl\"\n",
    "    f = open(datadir + pklname, \"wb\")\n",
    "    pickle.dump(datadict,f)\n",
    "    f.close()\n",
    "    print(\"Saved to:\", datadir + pklname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing timeseries data for plotting\n",
    "# Load existing data like this:\n",
    "#load_from = \"/storage/user/GenBank/frames/United Kingdom_dec01/data_920481519.pkl\"\n",
    "#datadict = pickle.load( open(load_from, \"rb\" ) )\n",
    "\n",
    "\n",
    "ddh = datadict[\"hammingdistributions\"]\n",
    "distributions = datadict[\"hammingdistributions\"] # Synonym\n",
    "ddd = datadict[\"dateranges\"]\n",
    "means = []\n",
    "variances = []\n",
    "deciles = []\n",
    "begdates = []\n",
    "enddates = []\n",
    "\n",
    "print(\"len(ddh) =\", len(ddh))\n",
    "\n",
    "for i in range(1,10):\n",
    "    deciles.append([])\n",
    "i_s = []\n",
    "for i in range(len(ddh)):\n",
    "    means.append(np.mean(ddh[i]))\n",
    "    variances.append(np.var(ddh[i]))\n",
    "    for j in range(1,10):\n",
    "        pctile = np.percentile(ddh[i], j*10)\n",
    "        deciles[j-1].append(pctile)\n",
    "    i_s.append(i)\n",
    "    begdates.append(ddd[i][0])\n",
    "    enddates.append(ddd[i][1])\n",
    "\n",
    "print(\"len(begdates)=\", len(begdates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figures (assumes data analysis is already done):\n",
    "for i in range(len(ddh)):\n",
    "    date_beg_str = ddd[i][0]\n",
    "    date_end = ddd[i][1]\n",
    "    dists = ddh[i]\n",
    "    __, __, __ = plt.hist(dists, range=[0, 160], bins=161, density=True)\n",
    "    plt.xlim([0,160])\n",
    "    plt.ylim([0,0.1])\n",
    "    plt.xlabel(\"Hamming distance\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(date_beg_str)\n",
    "    plt.savefig(datadir + str(i) + \".png\", dpi=175, facecolor='white', transparent=False, bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = [datetime.datetime.strptime(d,\"%Y-%m-%d\").date() for d in begdates]\n",
    "dec_idx=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean and median Hamming distances (as timeseries)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "fontsize=10\n",
    "\n",
    "font = {'family' : 'sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : fontsize}\n",
    "\n",
    "mystyle = 'seaborn'\n",
    "\n",
    "plt.style.use(mystyle)\n",
    "plt.rc('font', **font)\n",
    "\n",
    "\n",
    "fig, ((ax1)) = plt.subplots(1, 1, dpi=275, figsize=(6,3.8))\n",
    "\n",
    "\n",
    "cm = plt.cm.get_cmap('inferno')\n",
    "\n",
    "dcol = 0.25\n",
    "col0 = 0 # Color offset\n",
    "\n",
    "# Set colours:\n",
    "# Normal order\n",
    "col_multipliers = [1, 2, 2.8]\n",
    "# Reversed order:\n",
    "col_reversed = True\n",
    "if col_reversed:\n",
    "    col_multipliers = list(reversed(col_multipliers))\n",
    "col1 = cm(col0 + col_multipliers[0]*dcol)\n",
    "col2 = cm(col0 + col_multipliers[1]*dcol)\n",
    "col3 = cm(col0 + col_multipliers[2]*dcol)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot_date(begdates, means, fmt='', label=\"Mean\", color=col1)\n",
    "plt.plot_date(begdates, deciles[dec_idx], fmt='--', label=f\"{(dec_idx+1)*10}% percentile\", color=col2)\n",
    "\n",
    "#plt.xlabel(\"Time (generations)\")\n",
    "plt.ylabel(\"Hamming distance\")\n",
    "\n",
    "ticklist = []\n",
    "for i in range(len(begdates)):\n",
    "    if begdates[i][-2:]==\"01\":\n",
    "        if datetime.datetime.strptime(begdates[i],\"%Y-%m-%d\").month % 2==0:\n",
    "            pass # Skip even months\n",
    "        else:\n",
    "            ticklist.append(begdates[i])\n",
    "            print(\"Adding tick\", ticklist[-1])\n",
    "plt.xticks(ticklist, rotation=90)\n",
    "\n",
    "print(\"First day:\", begdates[0])\n",
    "print(\"Last day:\", begdates[-1])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim([-1,978])\n",
    "print(begdates[978])\n",
    "plt.ylim([0,145])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to matrix:\n",
    "\n",
    "ddh = distributions\n",
    "\n",
    "x_max = 200\n",
    "ny = len(ddh) # time\n",
    "nx = x_max\n",
    "plotmat = np.zeros((ny,nx))\n",
    "for i in range(ny):\n",
    "    hist_n, hist_bins = np.histogram(ddh[i], density=True, range=[0, x_max], bins=nx)\n",
    "    plotmat[i,:] = deepcopy(hist_n)\n",
    "    print(f\"t={i+1} out of {ny}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-radio",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Heatmap\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "fontsize=10\n",
    "\n",
    "font = {'family' : 'sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : fontsize}\n",
    "\n",
    "\n",
    "#mystyle = 'seaborn-notebook'\n",
    "mystyle = 'seaborn'\n",
    "\n",
    "# bmh is quite good somehow, but maybe too heavy\n",
    "plt.style.use(mystyle)\n",
    "plt.rc('font', **font)\n",
    "#plt.style.use(\"ggplot\")\n",
    "\n",
    "fig, ((ax)) = plt.subplots(1, 1, dpi=175, figsize=(6,8))\n",
    "\n",
    "#last_idx = 793\n",
    "last_idx = 978\n",
    "\n",
    "#showmat = np.log(0.02+plotmat[:936,:175]) \n",
    "showmat = np.log(0.02+plotmat[:last_idx,:175]) \n",
    "\n",
    "cax = ax.imshow(showmat, interpolation='nearest', aspect='auto', cmap=plt.get_cmap('inferno'))\n",
    "\n",
    "cbar = fig.colorbar(cax, ticks=[np.min(showmat[:,:]), np.max(showmat[:,:])])\n",
    "cbar.ax.set_yticklabels(['0', '1'])  # vertically oriented colorbar\n",
    "cbar.set_label('Frequency (logarithmic) within generation', rotation=270)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "#country_name = \"UK\"\n",
    "#country_name = \"USA\"\n",
    "country_name = country\n",
    "#country_name = \"Europe\"\n",
    "\n",
    "startdate = \"2020-03-01\"\n",
    "\n",
    "#plt.xlabel(\"Hamming distance (nucleotides)\")\n",
    "plt.xlabel(\"Hamming distance\")\n",
    "#plt.ylabel(f\"Day (since {startdate})\")\n",
    "plt.title(f\"Hamming heatmap ({country_name})\")\n",
    "\n",
    "#ax.set_yticks(np.arange(0,936))\n",
    "#ax.set_yticklabels(begdates)\n",
    "\n",
    "ticklist = []\n",
    "ticklabellist = []\n",
    "for i in range(len(begdates)):\n",
    "    if begdates[i][-2:]==\"01\":\n",
    "        if datetime.datetime.strptime(begdates[i],\"%Y-%m-%d\").month % 2==0:\n",
    "            pass # Skip even months\n",
    "        else:\n",
    "            ticklist.append(i)\n",
    "            ticklabellist.append(begdates[i])\n",
    "            print(\"Adding tick\", ticklist[-1])\n",
    "        \n",
    "__ = plt.yticks(ticklist, labels=ticklabellist)\n",
    "\n",
    "print(\"Last plotted range is\", begdates[last_idx], \"to\", enddates[last_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
